{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632cb43d-8681-47c7-b168-310fe408b359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          1001.0    1003.0    1005.0  ...    1015.0    1017.0    1019.0\n",
      "1001.0  0.000000  0.850792  0.783337  ...  0.891347  0.770100  0.831535\n",
      "1003.0  0.850792  0.000000  0.961628  ...  0.979194  0.924617  0.955950\n",
      "1005.0  0.783337  0.961628  0.000000  ...  0.937527  0.933636  0.938147\n",
      "1007.0  0.859640  0.964526  0.928717  ...  0.966785  0.882176  0.947517\n",
      "1009.0  0.879769  0.958348  0.912884  ...  0.975397  0.891829  0.948447\n",
      "1011.0  0.748210  0.909740  0.916322  ...  0.888671  0.889004  0.842882\n",
      "1013.0  0.906328  0.944460  0.900017  ...  0.940542  0.905915  0.909217\n",
      "1015.0  0.891347  0.979194  0.937527  ...  0.000000  0.889984  0.964386\n",
      "1017.0  0.770100  0.924617  0.933636  ...  0.889984  0.000000  0.876136\n",
      "1019.0  0.831535  0.955950  0.938147  ...  0.964386  0.876136  0.000000\n",
      "\n",
      "[10 rows x 10 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 112\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start_col \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m!=\u001b[39m j:\n\u001b[1;32m    111\u001b[0m     N_j \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:,j]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 112\u001b[0m     max_corr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(time_delayed_cross_correlation(N_i, N_j, max_tau)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_corr \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold:\n\u001b[1;32m    114\u001b[0m         partial_result[i,j] \u001b[38;5;241m=\u001b[39m max_corr\n",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m, in \u001b[0;36mtime_delayed_cross_correlation\u001b[0;34m(N_i, N_j, max_tau)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     48\u001b[0m     numerator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(shifted_i[valid_indices] \u001b[38;5;241m*\u001b[39m N_j[valid_indices]) \u001b[38;5;241m-\u001b[39m mean_i \u001b[38;5;241m*\u001b[39m mean_j\n\u001b[0;32m---> 49\u001b[0m     denominator \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(shifted_i[valid_indices]) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(N_j[valid_indices])\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Handle the case where the denominator is zero\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m denominator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3645\u001b[0m, in \u001b[0;36mstd\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3642\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3643\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m std(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_std(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof,\n\u001b[1;32m   3646\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:206\u001b[0m, in \u001b[0;36m_std\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_std\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, ddof\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    205\u001b[0m          where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 206\u001b[0m     ret \u001b[38;5;241m=\u001b[39m _var(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, ddof\u001b[38;5;241m=\u001b[39mddof,\n\u001b[1;32m    207\u001b[0m                keepdims\u001b[38;5;241m=\u001b[39mkeepdims, where\u001b[38;5;241m=\u001b[39mwhere)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, mu\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    210\u001b[0m         ret \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39msqrt(ret, out\u001b[38;5;241m=\u001b[39mret)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/numpy/core/_methods.py:173\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    168\u001b[0m     arrmean \u001b[38;5;241m=\u001b[39m arrmean \u001b[38;5;241m/\u001b[39m rcount\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Compute sum of squared deviations from mean\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Note that x may not be inexact and that we need it to be an array,\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# not a scalar.\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m x \u001b[38;5;241m=\u001b[39m asanyarray(arr \u001b[38;5;241m-\u001b[39m arrmean)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(arr\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, (nt\u001b[38;5;241m.\u001b[39mfloating, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[1;32m    176\u001b[0m     x \u001b[38;5;241m=\u001b[39m um\u001b[38;5;241m.\u001b[39mmultiply(x, x, out\u001b[38;5;241m=\u001b[39mx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import correlate\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "\n",
    "# Load the temporal series data\n",
    "data_with_date = pd.read_csv('temporal_series_20.csv')\n",
    "data = data_with_date.drop(columns=['date'])\n",
    "\n",
    "\n",
    "\n",
    "def time_delayed_cross_correlation(N_i, N_j, max_tau):\n",
    "    \"\"\"\n",
    "    Computes the time-delayed cross-correlation function for two time series N_i and N_j.\n",
    "    \n",
    "    Args:\n",
    "    N_i (numpy array): Time series for node i.\n",
    "    N_j (numpy array): Time series for node j.\n",
    "    max_tau (int): Maximum time delay (positive or negative).\n",
    "    \n",
    "    Returns:\n",
    "    cross_corrs (numpy array): Cross-correlation values for each time delay in the range [-max_tau, max_tau].\n",
    "    time_lags (numpy array): Corresponding time lags.\n",
    "    \"\"\"\n",
    "    n = len(N_i)\n",
    "    mean_i = np.mean(N_i)\n",
    "    mean_j = np.mean(N_j)\n",
    "    \n",
    "    # Prepare arrays for cross-correlations and time lags\n",
    "    cross_corrs = []\n",
    "    time_lags = range(-max_tau, max_tau + 1)\n",
    "    \n",
    "    for tau in time_lags:\n",
    "        if tau < 0:\n",
    "            shifted_j = np.roll(N_j, tau)  # shift N_j forward (N_j(t+τ))\n",
    "            valid_indices = np.arange(-tau, n)  # indices where the time shift is valid\n",
    "        else:\n",
    "            shifted_i = np.roll(N_i, -tau)  # shift N_i backward (N_i(t-τ))\n",
    "            valid_indices = np.arange(0, n - tau)  # indices where the time shift is valid\n",
    "\n",
    "        # Calculate the cross-correlation for the valid time points\n",
    "        if tau < 0:\n",
    "            numerator = np.mean(N_i[valid_indices] * shifted_j[valid_indices]) - mean_i * mean_j\n",
    "            denominator = np.std(N_i[valid_indices]) * np.std(shifted_j[valid_indices])\n",
    "        else:\n",
    "            numerator = np.mean(shifted_i[valid_indices] * N_j[valid_indices]) - mean_i * mean_j\n",
    "            denominator = np.std(shifted_i[valid_indices]) * np.std(N_j[valid_indices])\n",
    "        \n",
    "        # Handle the case where the denominator is zero\n",
    "        if denominator != 0:\n",
    "            corr = numerator / denominator\n",
    "        else:\n",
    "            corr = 0\n",
    "        \n",
    "        cross_corrs.append(corr)    \n",
    "    \n",
    "    return np.array(cross_corrs), np.array(time_lags) \n",
    "\n",
    "\n",
    "#We have a problem because sround 3000 columns is too much, first let's see what happens with a subset\n",
    "#Now we will make a test for the first 50 columns\n",
    "time_series_subset = data.iloc[:, :10]\n",
    "\n",
    "#Inizialize parameters\n",
    "max_tau = 15 \n",
    "threshold = 0.05\n",
    "\n",
    "#Let's compute the cross correlation function for each pair of columns in the subset\n",
    "subset_columns = time_series_subset.columns\n",
    "#To store results\n",
    "adjacency_matrix = np.zeros((len(subset_columns), len(subset_columns)))\n",
    "                            \n",
    "for i in range(len(subset_columns)):\n",
    "        for j in range(len(subset_columns)):\n",
    "            if i != j:\n",
    "                N_i = time_series_subset.iloc[:,i].values\n",
    "                N_j = time_series_subset.iloc[:,j].values\n",
    "                cross_corrs, _ = time_delayed_cross_correlation(N_i, N_j, max_tau)\n",
    "                #Now we store the results\n",
    "                max_corr = np.max(cross_corrs)\n",
    "                if max_corr >= threshold:\n",
    "                    adjacency_matrix[i,j] = max_corr\n",
    "\n",
    "\n",
    "adj_data = pd.DataFrame(adjacency_matrix, index=subset_columns, columns=subset_columns)\n",
    "print(adj_data)\n",
    "\n",
    "#Okkei now we try another approach, batch processing, we divide the computation in different parts, and then merge the results.\n",
    "\n",
    "#Initialize the dictionary to store batch results\n",
    "batch_results = {}\n",
    "\n",
    "#Adjust batch size based on available memory\n",
    "batch_size = 100\n",
    "\n",
    "#Process columns in batches\n",
    "for start_col in range(0, len(data.columns), batch_size):\n",
    "    end_col = min(start_col + batch_size, len(data.columns))\n",
    "    batch_columns = data.columns[start_col:end_col]\n",
    "\n",
    "    #Initialize a partial results matrix for this batch\n",
    "    partial_result = np.zeros((len(batch_columns),len(data.columns)))\n",
    "\n",
    "    #Compute cross-correlation for each pair in this batch\n",
    "    for i, col_i in enumerate(batch_columns):\n",
    "        N_i = data[col_i].values\n",
    "        for j in range(len(data.columns)):\n",
    "            if start_col + i != j:\n",
    "                N_j = data.iloc[:,j].values\n",
    "                max_corr = np.max(time_delayed_cross_correlation(N_i, N_j, max_tau)[0])\n",
    "                if max_corr >= threshold:\n",
    "                    partial_result[i,j] = max_corr\n",
    "\n",
    "    batch_results[start_col] = partial_result\n",
    "\n",
    "final_matrix = np.zeros((len(data.columns), len(data.columns)))\n",
    "for start_col, partial_result in batch_results.items():\n",
    "    end_col = start_col + partial_result.shape[0]\n",
    "    final_matrix[start_col:end_col, :] = partial_result\n",
    "# Convert the final matrix to a DataFrame\n",
    "final_df = pd.DataFrame(final_matrix, index=data.columns, columns=data.columns)\n",
    "final_df.to_csv(\"thresholded_cross_correlation_matrix.csv\")  # Save the final result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
